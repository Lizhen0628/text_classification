# global parameters
global_parameters:
  # model related
  - &model_arch_type 'TransformersRNN'
  - &is_train true #[true,false] # 如果不使用transformers model，该参数表示是否训练词向量，如果使用transformers model，该参数表示是否对transformers model 进行微调
  - &class_num 6
  # data related
  - &dataset_type 'MedicalTransformersDataset'
  - &data_dir 'data/medical_question/train_valid' # 定义锚
  - &cache_dir 'data/medical_question/.cache'
  - &overwrite_cache false   # 是否覆盖已经处理好的数据缓存。当数据处理耗费时间较长时，通过配置该参数为false，直接加载处理好的数据缓存。从而节省不必要的时间浪费
  - &transformer_model 'clue/albert_chinese_tiny' # 详情参见： https://huggingface.co/models
  - &force_download false
  - &num_workers 32
  - &batch_size 64


experiment_name: *model_arch_type
num_gpu: 2                         # GPU数量
device_id: '0,1'
visual_device: '0,1'
main_device_id: '0'
resume_path: null                         # path to latest checkpoint

# 模型
model_arch:
  type: *model_arch_type
  args:
    transformer_model: *transformer_model
    cache_dir: *cache_dir
    force_download: *force_download
    is_train: *is_train
    class_num: *class_num
    # rnn configure
    rnn_type: 'lstm'  # [rnn,lstm,gru]
    hidden_dim: 256
    n_layers: 2
    bidirectional: true
    batch_first: true
    dropout: 0.5

train_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'medical_train.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

valid_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'medical_valid.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker


optimizer:
  type: 'Adam'
  args:
    lr: 0.0001

lr_scheduler:
  type: 'get_linear_schedule_with_warmup'
  args:
    num_warmup_steps: 0

loss:
  - "binary_loss"

metrics:
  - 'accuracy'
  - 'macro_precision'
  - 'macro_recall'
  - 'macro_f1'
  - 'micro_precision'
  - 'micro_recall'
  - 'micro_f1'
trainer:
  epochs: 100
  save_dir: 'saved/'
  save_period: 1
  verbosity: 2
  monitor: "max val_macro_f1"
  early_stop: 20
  tensorboard: true

